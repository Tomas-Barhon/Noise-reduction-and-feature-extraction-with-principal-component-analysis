\chapter{Results and Discussion}
\label{chap:five}
In this section we will discuss the results obtained from 
the performed experiments. We particularly discuss the 
overfitting phenomenon, which is a common problem
for small sample sizes and for low signal to noise problems. 


\section{Results Intepretation}
\label{sec:results}
Initially, we intend to focus mainly on the price forecasting problem.
After running many experiments with unsatisfactory results, we decided to add 
a set of technical indicators calculated from the price variable.
Concretely, we added the following indicators: simple moving average (SMA),
exponential moving average (EMA), relative strength index (RSI) and 
bollinger bands (BB). 
The reason for this decision being the presence
of strong autocorrelation in the target variable which
might be a signal of limited impact of the fundamental pricing side. 
In order to assess potential variable importance
we have evaluated a correlation matrix for the initial
dataset, please see Figure \ref{fig:Corr_btc}. High 
levels of multicollinearity highlight the importance of
regularization techniques to avoid overfitting.

\begin{figure}[!h]
    \centering
    \caption{Correlation matrix of the BTC dataset shows high level of 
    multicollinearity.}
    \includegraphics[width=1\textwidth]{Figures/Corr_btc.pdf}
    \caption*{Source: Author}
    \label{fig:Corr_btc}
\end{figure}

\newpage
\begin{equation}\label{eq:sma}
    \text{SMA}_t = \frac{1}{w} \sum_{i=0}^{w-1} P_{t-i}
\end{equation}

\begin{equation}\label{eq:ema}
    \text{EMA}_t = \alpha P_t + (1 - \alpha) \text{EMA}_{t-1}, \quad \text{where} \quad \alpha = \frac{2}{w + 1}
\end{equation}

\begin{equation}\label{eq:rsi_avg}
    \text{AvgGain}_t = \frac{1}{w} \sum_{i=1}^{w} \max(\Delta P_{t-i}, 0), \quad
    \text{AvgLoss}_t = \frac{1}{w} \sum_{i=1}^{w} \max(-\Delta P_{t-i}, 0)
\end{equation}

\begin{equation}\label{eq:rsi}
    \text{RS}_t = \frac{\text{AvgGain}_t}{\text{AvgLoss}_t}, \quad
    \text{RSI}_t = 100 - \left( \frac{100}{1 + \text{RS}_t} \right)
\end{equation}

\begin{equation}\label{eq:bbands}
    \text{Upper}_t = \text{SMA}_t + k \cdot \sigma_t, \quad
    \text{Lower}_t = \text{SMA}_t - k \cdot \sigma_t
\end{equation}

After the addition of the technical indicators, the results
did not improve significantly and the model 
was always converging to the prediction of the current price
which leads to a typical pattern in the 
graph when putting the predicted and actual prices into comparison 
where the prediction is essentially
the target price shifted by the prediction horizon.
We test this hypothesis by comparing
the model with the naive forecast which predicts the last
observed price and charaterizes
the random walk model. The naive forecast outperforms 
the model in most of the cases on the test dataset. 


In the following steps we decided to focus on the prediction
of log returns. The log returns are calculated as follows:
\begin{equation}\label{eq:log_returns}
    r_t = \log(P_{t+h}) - \log(P_t) = \log\left(\frac{P_{t+h}}{P_t}\right)
\end{equation}

where $P_t$ is the price at time $t$ and $h$ is the time horizon.
The idea behind this approach is that the log returns are stationary in nature
and the model can thus better capture
the pricing dynamics without learning the long term trend.
Due to log returns stationarity, 
$R^2$ is a reasonable measure of the model performance that
tells whether the model is better than the naive mean forecast.
Positive $R^2$ indicates that the model is better than the mean forecast.
Technically, $R^2$ is a relatively strict measure
when the returns shift systematically in one direction on the test dataset.
The reason is that the sum of squares total
is calculated on the test data and thus knowing the test data mean.
However, the models trained on the training dataset do not 
capture this shift as they do not know the test data mean.
Despite that we believe that $R^2$ is a reasonable measure
as the models should be able to outperform this baseline.
Unfortunately, this problem reformulation 
did not help much and the model was still
unable to generalize well to the test dataset despite
decent performance on the training dataset.
It is worth noting that most of our
models have lower root mean square error on the test
dataset than on the training dataset so they 
are not truly overfitting the training data.



We suggest multiple reasons for this behavior. 
Either the signal to noise ratio is too low
and the model is not able 
to learn the pricing dynamics. 
The second reason is that the model is overfitting the training
data despite extremely strong regularization of many forms and despite
cross validation. We suspect
that the combination of the small sample size and the
nature of the cross validation 
are the main reasons for this behaviour. 

For time series data, the temporal ordering of the data is important.
That is the reason why one should not use the standard cross validation
in order to avoid data leakage especially 
for the scalers that scale the data into a range
which is required by some models. In practice, 
one can use normal cross validation on the training dataset
but it will most likely lead to overfitting.
The second option is to use the time series cross validation
which incrementally increases the training dataset 
and evaluates the model on the next batch of the data.
This approach is more robust but has its own limitations.
The first one is that when the sample size is small, 
the performance of the model is very noisy across the folds
which makes it difficult to evaluate the best hyperparameters. 
Furthermore, varying sample size actually 
leads to different number of gradient descent iterations
for some models which affects the choice of the best hyperparameters.
The second weakness is actually the reasons
that makes this approach more robust and that is the
act of masking the future distribution of the data.
As most machine learning models and certainly 
those used in our experiments including the \ac{PCA} step
expect the data to be standardized
or scaled to some predefined range.
When the distribution of the data changes
significantly, the scalers trained on the training dataset
will cause the test data to be scaled
outside of those expected ranges and thus breaking the 
performance of the model. 
Unfortunately,
this is the nature of our data 
and shows some limitations of the machine learning approach 
for problems that are changing over time.


Due to afforementioned problems, we opted for a different approach
which includes differencing and applying logarithm to all of
the independent variables to push them closer to stationarity.
This obviously leads to loss of information 
but may help with the cross validation problem.
Especially, variables that come in weekly and monthly frequencies
and are thus interpolated to daily frequency
will be zero most of the time and thus
may introduce a lot of variance on the edges where the values
are changing. For the correlation matrix of this transformed 
dataset see Figure \ref{fig:Corr_btc_logdiff}. We can 
observe that the correlation levels between the independent variables
and the target variable are really low with highest 
levels being with the technical indicators.

\begin{figure}[!h]
    \centering
    \caption{Correlation matrix of the BTC dataset after
    log differencing all of the variables.}
    \includegraphics[width=1\textwidth]{Figures/Corr_btc_logdiff.pdf}
    \caption*{Source: Author}
    \label{fig:Corr_btc_logdiff}
\end{figure}

After changing the model specification, we repeated the experiments
using Bayesian cross validation which is 
an advanced technique that 
usually converges faster to better hyperparameters than the standard
cross validation. 
We studied the effect of hyperparamters on the model performance
and found that the results are mostly insensitive to the choice of the
hyperparameters. Similarly, cross validation tends to 
select highly regularized models that 
are not able to learn the training data well and only predict the mean value.
This is a typical sign of overfitting to the training noise and
not to the signal. 


As cryptomarket is characterized by significant structural breaks
that shift the distribution of the data, we wanted
to test whether the model is able to generalize well
until the highly volatile period of 2021. We decided to cut 
the dataset at the beginning of 2021 and rerun the experiments.
Howevery, the results were not significantly different and the 
generalization performance was still very poor.


We suspect that the signal to noise ratio is too low
or the the pricing dynamics are changing too often 
which leads to the inability of the model to generalize
to out-of-sample time periods. We inspect the latter hypothesis
on the example of the learned coefficients of the Ridge regression model
with incremental training that represents
the time series split and the sliding window approach that represents
the true changes in the dynamics in time.
The learned coefficients are shown 
in Figures \ref{fig:coefs_incremental_btc} and \ref{fig:coefs_sliding_btc},
where the coefficients with highest variance are highlighted.
We can observe that the coefficients are changing over time relatively
dramatically which explains the poor performance of the model. Obviously,
some variance is expected due to the low size of these folds
that causes noisiness in the learned coefficients especially in the sliding
window approach. Similar results were obtained for the cryptocurrencies.
To see the final results tables see Appendix \ref{app:A}.


\begin{figure}[!h]
    \centering
    \caption{Learned coefficients of the Ridge regression model
    with incremental training on the BTC dataset. Five 
    coefficients with highest variance are highlighted.}
    \includegraphics[width=1\textwidth]{Figures/coefficient_values_incremental_btc.pdf}
    \caption*{Source: Author}
    \label{fig:coefs_incremental_btc}
\end{figure}

\begin{figure}[!h]
    \centering
    \caption{Learned coefficients of the Ridge regression model
    with sliding window training on the BTC dataset. Five 
    coefficients with highest variance are highlighted.}
    \includegraphics[width=1\textwidth]{Figures/coefficient_values_sliding_btc.pdf}
    \caption*{Source: Author}
    \label{fig:coefs_sliding_btc}
\end{figure}

\section{Limitations}
\label{sec:limitations}
Unfortunately, the poor model performance 
prevents us from making any conclusions about the
initial hypothesis of the thesis. The results 
are not significantly different even with the \ac{PCA}
layer added. However, there seems
to be some limited evidence for less overfitting behaviour 
in the case of the \ac{PCA} model
but the results are still worse than the naive forecast leading to
negative $R^2$ on the test dataset. We conclude
that the model is not able to learn the pricing dynamics
due to the low signal to noise ratio and the changing pricing dynamics.


Our study is limited by the choice and 
quality of the data. Despite having 
a lot of variables representing the network activity and 
other fundamental variables, the quality of the trend data is low.
Only the wikipedia page views come in daily frequency
but we did not expect this variable to be highly informative. 
Whereas Google trends data was available in weekly frequency
which leads to low information for daily forecasting.
We believe that better data quality would improve
the model performance significantly. 
Secondly, there might exist a better combination of technical
indicators that would help the model to learn the pricing dynamics.
Finally, more advanced techniques to increase
the dataset size such as generative adversarial networks 
might help to improve the generalizability properties of the model.